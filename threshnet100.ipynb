{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexa\\mambaforge\\envs\\triplenet\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "c:\\Users\\alexa\\mambaforge\\envs\\triplenet\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> epoch: 1/200\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 69ms | Tot: 34s158ms | Loss: 1.3298 | Acc: 51.412% (25706/50000)=======>.....................................................................]  Step: 68ms | Tot: 4s464ms | Loss: 1.7577 | Acc: 34.561% (2281/6600) [==================================>.............................................]  Step: 69ms | Tot: 14s791ms | Loss: 1.5424 | Acc: 43.065% (9345/21700) [========================================>.......................................]  Step: 70ms | Tot: 17s325ms | Loss: 1.5008 | Acc: 44.665% (11345/25400)\n",
      "(664.8984414339066, 0.51412)\n",
      "test:\n",
      " 100/100 [================================================================================>]  Step: 20ms | Tot: 2s28ms | Loss: 1.1243 | Acc: 59.610% (5961/10000)\n",
      "\n",
      "===> epoch: 2/200\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 68ms | Tot: 34s153ms | Loss: 0.9101 | Acc: 68.090% (34045/50000)======================>........................................................]  Step: 69ms | Tot: 9s937ms | Loss: 0.9717 | Acc: 65.432% (9553/14600) [============================================================>...................]  Step: 68ms | Tot: 25s944ms | Loss: 0.9304 | Acc: 67.211% (25540/38000)\n",
      "(455.06418138742447, 0.6809)\n",
      "test:\n",
      " 100/100 [================================================================================>]  Step: 20ms | Tot: 2s36ms | Loss: 0.8379 | Acc: 71.690% (7169/10000)\n",
      "\n",
      "===> epoch: 3/200\n",
      "train:\n",
      " 244/500 [=======================================>........................................]  Step: 69ms | Tot: 16s808ms | Loss: 0.7525 | Acc: 74.164% (18096/24400)====>.........................................................................]  Step: 70ms | Tot: 2s848ms | Loss: 0.7370 | Acc: 74.810% (3142/4200) [================>...............................................................]  Step: 69ms | Tot: 7s158ms | Loss: 0.7541 | Acc: 73.663% (7661/10400)\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\OneDrive\\01 Frankfurt School\\Semester 3\\Deep Learning\\TripleNetDL\\cifar10Threshnet\\main.py:147\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave()\n\u001b[0;32m    146\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 147\u001b[0m     main()\n",
      "File \u001b[1;32m~\\OneDrive\\01 Frankfurt School\\Semester 3\\Deep Learning\\TripleNetDL\\cifar10Threshnet\\main.py:33\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m args \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39mparse_args()\n\u001b[0;32m     32\u001b[0m solver \u001b[39m=\u001b[39m Solver(args)\n\u001b[1;32m---> 33\u001b[0m solver\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[1;32m~\\OneDrive\\01 Frankfurt School\\Semester 3\\Deep Learning\\TripleNetDL\\cifar10Threshnet\\main.py:138\u001b[0m, in \u001b[0;36mSolver.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheduler\u001b[39m.\u001b[39mstep(epoch)\n\u001b[0;32m    137\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m===> epoch: \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m/200\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m epoch)\n\u001b[1;32m--> 138\u001b[0m train_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m    139\u001b[0m \u001b[39mprint\u001b[39m(train_result)\n\u001b[0;32m    140\u001b[0m test_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest()\n",
      "File \u001b[1;32m~\\OneDrive\\01 Frankfurt School\\Semester 3\\Deep Learning\\TripleNetDL\\cifar10Threshnet\\main.py:86\u001b[0m, in \u001b[0;36mSolver.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(data)\n\u001b[0;32m     85\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(output, target)\n\u001b[1;32m---> 86\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     87\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     88\u001b[0m iter_loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\alexa\\mambaforge\\envs\\triplenet\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\alexa\\mambaforge\\envs\\triplenet\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%run cifar10threshnet\\main.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triplenet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
