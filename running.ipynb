{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexa\\mambaforge\\envs\\triplenet\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "c:\\Users\\alexa\\mambaforge\\envs\\triplenet\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> epoch: 1/200\n",
      "train:\n",
      " 782/782 [================================================================================>]  Step: 626ms | Tot: 50s848ms | Loss: 1.5846 | Acc: 40.000% (20000/50000)======================>........................................................]  Step: 65ms | Tot: 14s526ms | Loss: 1.9192 | Acc: 26.294% (3820/14528) [=================================>..............................................]  Step: 66ms | Tot: 21s250ms | Loss: 1.8429 | Acc: 28.868% (6097/21120)\n",
      "(1239.1229352355003, 0.4)\n",
      "test:\n",
      " 157/157 [================================================================================>]  Step: 14ms | Tot: 2s779ms | Loss: 1.3349 | Acc: 49.880% (4988/10000)\n",
      "\n",
      "===> epoch: 2/200\n",
      "train:\n",
      " 782/782 [================================================================================>]  Step: 62ms | Tot: 49s403ms | Loss: 1.0642 | Acc: 62.218% (31109/50000)[=============================>..................................................]  Step: 62ms | Tot: 18s418ms | Loss: 1.1604 | Acc: 58.355% (10756/18432)\n",
      "(832.1757242679596, 0.62218)\n",
      "test:\n",
      " 157/157 [================================================================================>]  Step: 14ms | Tot: 2s728ms | Loss: 1.1697 | Acc: 59.320% (5932/10000)\n",
      "\n",
      "===> epoch: 3/200\n",
      "train:\n",
      " 782/782 [================================================================================>]  Step: 65ms | Tot: 49s520ms | Loss: 0.8757 | Acc: 69.648% (34824/50000)[==================================>.............................................]  Step: 63ms | Tot: 21s428ms | Loss: 0.8988 | Acc: 68.584% (14836/21632) [====================================>...........................................]  Step: 66ms | Tot: 22s917ms | Loss: 0.8995 | Acc: 68.689% (15870/23104)\n",
      "(684.7741290032864, 0.69648)\n",
      "test:\n",
      " 157/157 [================================================================================>]  Step: 13ms | Tot: 2s736ms | Loss: 0.9142 | Acc: 67.900% (6790/10000)==================>..........................................................]  Step: 18ms | Tot: 732ms | Loss: 0.9191 | Acc: 67.411% (1812/2688)\n",
      "\n",
      "===> epoch: 4/200\n",
      "train:\n",
      " 782/782 [================================================================================>]  Step: 66ms | Tot: 49s544ms | Loss: 0.7677 | Acc: 73.712% (36856/50000).............................................................................]  Step: 63ms | Tot: 1s563ms | Loss: 0.7466 | Acc: 73.377% (1221/1664)\n",
      "(600.3718712031841, 0.73712)\n",
      "test:\n",
      " 157/157 [================================================================================>]  Step: 14ms | Tot: 2s737ms | Loss: 0.8128 | Acc: 72.620% (7262/10000)\n",
      "\n",
      "===> epoch: 5/200\n",
      "train:\n",
      " 782/782 [================================================================================>]  Step: 69ms | Tot: 48s965ms | Loss: 0.6770 | Acc: 77.202% (38601/50000)\n",
      "(529.3846656084061, 0.77202)\n",
      "test:\n",
      " 157/157 [================================================================================>]  Step: 13ms | Tot: 2s771ms | Loss: 0.7934 | Acc: 73.250% (7325/10000)\n",
      "\n",
      "===> epoch: 6/200\n",
      "train:\n",
      " 782/782 [================================================================================>]  Step: 66ms | Tot: 48s724ms | Loss: 0.6146 | Acc: 79.260% (39630/50000)[====================>...........................................................]  Step: 63ms | Tot: 12s479ms | Loss: 0.6357 | Acc: 78.836% (10091/12800)\n",
      "(480.62543922662735, 0.7926)\n",
      "test:\n",
      " 157/157 [================================================================================>]  Step: 14ms | Tot: 2s640ms | Loss: 0.7509 | Acc: 75.420% (7542/10000)\n",
      "\n",
      "===> epoch: 7/200\n",
      "train:\n",
      " 782/782 [================================================================================>]  Step: 64ms | Tot: 58s804ms | Loss: 0.5569 | Acc: 81.066% (40533/50000)===========>..................................................................]  Step: 68ms | Tot: 8s388ms | Loss: 0.5443 | Acc: 81.514% (7095/8704) [================================>...............................................]  Step: 65ms | Tot: 19s945ms | Loss: 0.5496 | Acc: 81.501% (16535/20288) [=================================>..............................................]  Step: 63ms | Tot: 20s715ms | Loss: 0.5501 | Acc: 81.478% (17156/21056) [========================================================>.......................]  Step: 98ms | Tot: 39s97ms | Loss: 0.5570 | Acc: 81.130% (28506/35136)\n",
      "(435.52574732899666, 0.81066)\n",
      "test:\n",
      " 157/157 [================================================================================>]  Step: 16ms | Tot: 3s55ms | Loss: 0.6948 | Acc: 77.700% (7770/10000)\n",
      "\n",
      "===> epoch: 8/200\n",
      "train:\n",
      " 782/782 [================================================================================>]  Step: 68ms | Tot: 50s464ms | Loss: 0.5066 | Acc: 82.786% (41393/50000)=======>......................................................................]  Step: 69ms | Tot: 8s15ms | Loss: 0.4697 | Acc: 83.810% (5042/6016)\n",
      "(396.1715970784426, 0.82786)\n",
      "test:\n",
      " 157/157 [================================================================================>]  Step: 13ms | Tot: 2s583ms | Loss: 0.6297 | Acc: 79.040% (7904/10000)\n",
      "\n",
      "===> epoch: 9/200\n",
      "train:\n",
      " 782/782 [================================================================================>]  Step: 63ms | Tot: 48s720ms | Loss: 0.4652 | Acc: 84.318% (42159/50000)\n",
      "(363.82192219793797, 0.84318)\n",
      "test:\n",
      " 157/157 [================================================================================>]  Step: 13ms | Tot: 2s681ms | Loss: 0.5967 | Acc: 80.490% (8049/10000)\n",
      "\n",
      "===> epoch: 10/200\n",
      "train:\n",
      " 782/782 [================================================================================>]  Step: 65ms | Tot: 48s858ms | Loss: 0.4247 | Acc: 85.778% (42889/50000)[=====================================>..........................................]  Step: 65ms | Tot: 23s160ms | Loss: 0.4218 | Acc: 85.942% (20351/23680)\n",
      "(332.13609974086285, 0.85778)\n",
      "test:\n",
      " 157/157 [================================================================================>]  Step: 14ms | Tot: 2s689ms | Loss: 0.6027 | Acc: 80.440% (8044/10000)\n",
      "\n",
      "===> epoch: 11/200\n",
      "train:\n",
      " 782/782 [================================================================================>]  Step: 64ms | Tot: 48s869ms | Loss: 0.3918 | Acc: 86.778% (43389/50000)\n",
      "(306.41434029489756, 0.86778)\n",
      "test:\n",
      " 157/157 [================================================================================>]  Step: 13ms | Tot: 2s653ms | Loss: 0.6097 | Acc: 80.820% (8082/10000)\n",
      "\n",
      "===> epoch: 12/200\n",
      "train:\n",
      " 782/782 [================================================================================>]  Step: 67ms | Tot: 48s829ms | Loss: 0.3602 | Acc: 88.098% (44049/50000)\n",
      "(281.69387897104025, 0.88098)\n",
      "test:\n",
      " 157/157 [================================================================================>]  Step: 13ms | Tot: 2s670ms | Loss: 0.5976 | Acc: 80.810% (8081/10000)\n",
      "\n",
      "===> epoch: 13/200\n",
      "train:\n",
      " 782/782 [================================================================================>]  Step: 74ms | Tot: 49s319ms | Loss: 0.3279 | Acc: 89.022% (44511/50000)[=======================>........................................................]  Step: 64ms | Tot: 14s511ms | Loss: 0.3188 | Acc: 89.230% (13306/14912) [=====================================================>..........................]  Step: 63ms | Tot: 32s872ms | Loss: 0.3252 | Acc: 89.104% (29939/33600)\n",
      "(256.39726626873016, 0.89022)\n",
      "test:\n",
      " 157/157 [================================================================================>]  Step: 14ms | Tot: 2s827ms | Loss: 0.5591 | Acc: 82.090% (8209/10000)\n",
      "\n",
      "===> epoch: 14/200\n",
      "train:\n",
      " 782/782 [================================================================================>]  Step: 64ms | Tot: 49s435ms | Loss: 0.3026 | Acc: 89.880% (44940/50000)[=================================================================>..............]  Step: 67ms | Tot: 40s745ms | Loss: 0.2998 | Acc: 89.993% (37149/41280) [==================================================================>.............]  Step: 64ms | Tot: 40s809ms | Loss: 0.2997 | Acc: 89.999% (37209/41344)\n",
      "(236.60248040407896, 0.8988)\n",
      "test:\n",
      " 157/157 [================================================================================>]  Step: 14ms | Tot: 2s771ms | Loss: 0.5575 | Acc: 82.360% (8236/10000)\n",
      "\n",
      "===> epoch: 15/200\n",
      "train:\n",
      " 782/782 [================================================================================>]  Step: 61ms | Tot: 49s75ms | Loss: 0.2717 | Acc: 90.772% (45386/50000)\n",
      "(212.48004608228803, 0.90772)\n",
      "test:\n",
      " 157/157 [================================================================================>]  Step: 13ms | Tot: 2s676ms | Loss: 0.5441 | Acc: 82.770% (8277/10000)\n",
      "\n",
      "===> epoch: 16/200\n",
      "train:\n",
      " 763/782 [==============================================================================>.]  Step: 64ms | Tot: 47s285ms | Loss: 0.2451 | Acc: 91.854% (44854/48832)\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\OneDrive\\01 Frankfurt School\\Semester 3\\Deep Learning\\TripleNetDL-1\\cifar10\\main.py:152\u001b[0m\n\u001b[0;32m    148\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave()\n\u001b[0;32m    151\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 152\u001b[0m     main()\n",
      "File \u001b[1;32m~\\OneDrive\\01 Frankfurt School\\Semester 3\\Deep Learning\\TripleNetDL-1\\cifar10\\main.py:31\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m args \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39mparse_args()\n\u001b[0;32m     30\u001b[0m solver \u001b[39m=\u001b[39m Solver(args)\n\u001b[1;32m---> 31\u001b[0m solver\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[1;32m~\\OneDrive\\01 Frankfurt School\\Semester 3\\Deep Learning\\TripleNetDL-1\\cifar10\\main.py:142\u001b[0m, in \u001b[0;36mSolver.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheduler\u001b[39m.\u001b[39mstep(epoch)\n\u001b[0;32m    141\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m===> epoch: \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m/200\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m epoch)\n\u001b[1;32m--> 142\u001b[0m train_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m    143\u001b[0m \u001b[39mprint\u001b[39m(train_result)\n\u001b[0;32m    144\u001b[0m test_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest()\n",
      "File \u001b[1;32m~\\OneDrive\\01 Frankfurt School\\Semester 3\\Deep Learning\\TripleNetDL-1\\cifar10\\main.py:80\u001b[0m, in \u001b[0;36mSolver.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     77\u001b[0m all_train_iter_loss \u001b[39m=\u001b[39m []\n\u001b[0;32m     78\u001b[0m train_loss_values \u001b[39m=\u001b[39m []\n\u001b[1;32m---> 80\u001b[0m \u001b[39mfor\u001b[39;00m batch_num, (data, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_loader):\n\u001b[0;32m     81\u001b[0m     data, target \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice), target\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     82\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\alexa\\mambaforge\\envs\\triplenet\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\alexa\\mambaforge\\envs\\triplenet\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    569\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    572\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32mc:\\Users\\alexa\\mambaforge\\envs\\triplenet\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\alexa\\mambaforge\\envs\\triplenet\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\alexa\\mambaforge\\envs\\triplenet\\lib\\site-packages\\torchvision\\datasets\\cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    115\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img)\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m    120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\alexa\\mambaforge\\envs\\triplenet\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\alexa\\mambaforge\\envs\\triplenet\\lib\\site-packages\\torchvision\\transforms\\transforms.py:135\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    128\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[1;32mc:\\Users\\alexa\\mambaforge\\envs\\triplenet\\lib\\site-packages\\torchvision\\transforms\\functional.py:153\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    151\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mview(pic\u001b[39m.\u001b[39msize[\u001b[39m1\u001b[39m], pic\u001b[39m.\u001b[39msize[\u001b[39m0\u001b[39m], \u001b[39mlen\u001b[39m(pic\u001b[39m.\u001b[39mgetbands()))\n\u001b[0;32m    152\u001b[0m \u001b[39m# put it from HWC to CHW format\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39;49mpermute((\u001b[39m2\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m))\u001b[39m.\u001b[39mcontiguous()\n\u001b[0;32m    154\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mByteTensor):\n\u001b[0;32m    155\u001b[0m     \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39mdefault_float_dtype)\u001b[39m.\u001b[39mdiv(\u001b[39m255\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%run cifar10\\main.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triplenet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
